---
title: "STAT 341: Assignment 2"
subtitle: "DUE: Friday, June 20, 2025 by 11:59pm EST"
output:
  pdf_document:
    keep_tex: yes
    number_sections: no
    latex_engine: xelatex
  html_document:
    toc: yes
  word_document: default
urlcolor: blue
---

```{r load-census-data, echo=FALSE}

census_data <- read.csv(
  "Stat341_S25_A2_Census_Data_CSD.csv",
  stringsAsFactors = FALSE
)
```


$\;$
$\;$
$\;$
$\;$

## NOTES

Your assignment must be submitted by the due date listed at the top of this document, and it must be submitted electronically in .pdf format via Crowdmark. This means that your responses for different questions should begin on separate pages of your .pdf file. Note that your .pdf solution file must have been generated by R Markdown. Additionally:

* For mathematical questions: your solutions must be produced by LaTeX (from within R Markdown). Neither screenshots nor scanned/photographed handwritten solutions will be accepted -- these will receive zero points.

* For computational questions: R code should always be included in your solution (via code chunks in R Markdown). If code is required and you provide none, you will receive zero points.

* For interpretation questions: plain text (within R Markdown) is required. Text responses embedded as comments within code chunks will not be accepted.

Organization and comprehensibility is part of a full solution. Consequently, points will be deducted for solutions that are not organized and incomprehensible. Furthermore, if you submit your assignment to Crowdmark, but you do so incorrectly in any way (e.g., you upload your Question 2 solution in the Question 1 box), you will receive a 5% deduction (i.e., 5% of the assignment’s point total will be deducted from your point total).

\newpage

## QUESTION 1 (10 marks): Loaded Dice and the non-centrality parameter


The deviation from expectation, $\sum (E-O)^2 / E$ of a fair $d$-sided die follows a standard $\chi^2$ distribution with  $d-1$ degrees of freedom, where the expected count is $1/d$ for each side. (For example, on a standard 6-sided die, we fit a chi-squared with 5 degrees of freedom)

However, a die that is UNFAIR will deviate from the expected fair values according to the **non-central** chi-squared. That is, where a chi-squared distribution has a non-centrality parameter $\theta > 0$.

Below is a plot of the chi-squared distribution with 5 and 20 degrees of freedom and a non-centrality parameter $\theta$ of 0, and 4.

```{r,fig.width=8, fig.height=5.5}
x = seq(0, 50, 0.1)
y5  = dchisq(x, df=5)
y20 = dchisq(x, df=20)

y5_nc  = dchisq(x, df=5, ncp=4)
y20_nc = dchisq(x, df=20, ncp=4)

plot(x, y5, lwd=3, type="l",
  main="Chi-squared distributions
  df=5 (Black), df=20 (Red),
  central = solid line, non-central = dashed")
lines(x, y20,    lwd=3, lty=1, col="Red")
lines(x, y5_nc,  lwd=3, lty=2, col="Black")
lines(x, y20_nc, lwd=3, lty=2, col="Red")
```


\newpage



a) (5 marks) Create a function `rho_ncp_fn = function(x=0, rollset)` to find the negative log-likelihood of $12000$ independent rolls of a fair die, and three different unfair dice. The arguments it takes in are `x`, the non-centrality parameter, and `rollset`, the set of rolls that are generated in the following code that makes `rolls_fair` and so on. You can use `dchisq()` instead of writing out the PDF.


Unfair die 1

$$p_1 = \frac{1}{12}, \ \ \ p_2 = \frac{2}{12}, \ \ \ p_3 = \frac{2}{12}, \ \ \ 
p_4 = \frac{2}{12}, \ \ \ p_5 = \frac{2}{12}, \ \ \ p_6 = \frac{3}{12}$$

Unfair die 2


$$p_1 = \frac{3}{24}, \ \ \ p_2 = \frac{4}{24}, \ \ \ p_3 = \frac{4}{24}, \ \ \ 
p_4 = \frac{4}{24}, \ \ \ p_5 = \frac{4}{24}, \ \ \ p_6 = \frac{5}{24}$$

Unfair die 3


$$p_1 = \frac{3}{12}, \ \ \ p_2 = \frac{3}{12}, \ \ \ p_3 = \frac{3}{12}, \ \ \ 
p_4 = \frac{1}{12}, \ \ \ p_5 = \frac{1}{12}, \ \ \ p_6 = \frac{1}{12}$$



```{r}
# Makes the rollset
set.seed(341)
rolls_fair =    sample(1:6, size=12000, replace=TRUE)
rolls_unfair1 = sample(1:6, size=12000, replace=TRUE, 
                       prob=c(1,2,2,2,2,3)/12)
rolls_unfair2 =  sample(1:6, size=12000, replace=TRUE, 
                       prob=c(3,4,4,4,4,5)/24)
rolls_unfair3 =  sample(1:6, size=12000, replace=TRUE, 
                       prob=c(3,3,3,1,1,1)/12)
```

```{r}
rho_ncp_fn <- function(x, rollset) {
  N   <- length(rollset)
  obs <- as.numeric(table(factor(rollset,1:6)))
  E   <- N/6
  chi2<- sum((obs - E)^2/E)
  ll  <- dchisq(chi2, df=5, ncp = x * N, log = TRUE)
  -ll
}


```



b) (2 marks) Use `optim(... method="Brent", lower=0, upper=10)` and the rho function you made in part b to find the maximum likelihood estimates of the function.


```{r}
optim(
  par     = 0.1,
  fn      = rho_ncp_fn,
  rollset = rolls_unfair1,
  method  = "Brent",
  lower   = 0,
  upper   = 1
)$par



```



c) (3 marks) The true value $\theta$ of the non-centrality parameter per observation is

$$ \theta = \sum_{i=1}^p \frac{(a_i - b_i)^2 }{b_i}$$
where $a_i$ is the true probability of that event happening (e.g., the unfair die roll probabilities), and $b_i$ is the assumed probability (e.g., 1/6 because we assume all dice are fair)

Find the $\theta$ for each of the three unfair dice, and compare your answers to what `optim()` has provided.

Source: https://en.wikipedia.org/wiki/Noncentral_chi-squared_distribution



```{r}

b <- rep(1/6, 6)

a1 <- c(1,2,2,2,2,3) / 12
a2 <- c(3,4,4,4,4,5) / 24
a3 <- c(3,3,3,1,1,1) / 12
theta_true <- c(
  un1 = sum((a1 - b)^2 / b),
  un2 = sum((a2 - b)^2 / b),
  un3 = sum((a3 - b)^2 / b)
)
theta_true

mle_per_obs <- c(
  un1 = optim(
           par     = 0.1,
           fn      = rho_ncp_fn,
           rollset = rolls_unfair1, 
           method  = "Brent",
           lower   = 0,
           upper   = 1
         )$par,

  un2 = optim(
           par     = 0.1,
           fn      = rho_ncp_fn,
           rollset = rolls_unfair2,
           method  = "Brent",
           lower   = 0,
           upper   = 1
         )$par,

  un3 = optim(
           par     = 0.1,
           fn      = rho_ncp_fn,
           rollset = rolls_unfair3,
           method  = "Brent",
           lower   = 0,
           upper   = 1
         )$par
)
mle_per_obs

data.frame(
  die         = names(theta_true),
  theoretical = theta_true,
  MLE         = mle_per_obs
)

```





\newpage

## QUESTION 2 (15 marks): Dog Agility Part 2 - Optimizing a Bezier Curve


This is part 2 of the project to analyze maps of dog agility courses and assess them for things like overall difficulty. In this first part, we're just going to be establishing the functions necessary to determine the physical dynamics of an arbitrary path.

In this part, we're going to use generalized optimization and gradient descent in order to estimate a likely path that a dog would take from one obstacle to another by optimizing cubic Bezier curves.


A cubic Bezier curve is defined by a starting point, an ending point and two guide points that are used for interpolation.

For example, the Bezier curve we examined in assignment 1 has a starting point of (,) an ending point of (,) and guide points of (,) and (,).


a) (3 marks) Given the following starting point, ending point, and second guide point (point2). Calculate the `rho` for each of the NA values left behind by the data frame made by this code. Report the `head()` and the `tail()` of `df_1a` for marking.

```{r}
# Makes the data frame to use
x = rep(seq(0,10,0.1), times=101)
y = rep(seq(0,10,0.1), each=101)
df_1a = data.frame(x,y)
df_1a$rho = NA

head(df_1a)
```


The easiest way to do this is to run a `for(k in 1:nrow(df_1a))` loop where `df_1a$rho[k]` is calculated by putting `c(df_1a$x[k], df_1a$y[k])` into `x` for `make_bezier_path_1free`, and putting the resulting `$x` and `$y` into `get_path_badness`.



```{r}

make_bezier_path_1free = function(x=c(3,5), point2=c(7,5), begin=c(0,0), end=c(10,2))
{
  px = c(begin[1], x[1], point2[1], end[1])
  py = c(begin[2], x[2], point2[2], end[2])
  
  t = seq(from=0, to=1, by=0.01)
  bezier_path_x = (1-t)^3*px[1] + 
                  3*(1-t)^2*t*px[2] +  
                  3*(1-t)*t^2*px[3] +  
                  t^3*px[4]
  bezier_path_y = (1-t)^3*py[1] + 
                  3*(1-t)^2*t*py[2] +  
                  3*(1-t)*t^2*py[3] +  
                  t^3*py[4]

  return(list(x=bezier_path_x, y=bezier_path_y))
}

```


Our badness/loss function is

$$\rho(\mathbf{x},\mathbf{y}) = a\times\mathrm{max}(acceleration)^2  + 
b\times(distance)^2 +  c\times\mathrm{max}(direction\ change)^2$$

Where the values $a=10$, $b=1$, and $c=20$ would ideally come from expert opinion 
such a dog agility judge, or a vet, but in this case they are arbitrarily made up.

The squared factor is to prevent any one factor from being maximized order to bring 
the other factors to zero. This function is calculated for a given 
$(\mathbf{x},\mathbf{y})$ with the following function:


```{r}

get_path_badness = function(path, 
                        factor_acc=10, factor_dist=1, factor_cdir=20)
{
  x = path$x
  y = path$y
  dx = diff(x)
  dy = diff(y)
  d2x = c(diff(dx), 0)
  d2y = c(diff(dy), 0)
  
  # [[ACCELERATION]]
  acc = sqrt(d2x^2 + d2y^2)
  
  # [[DISTANCE]]
  ds = sqrt(dx^2 + dy^2)

  # [[CHANGE IN DIRECTION]]
  dir = atan(dy/dx)
  ddir = abs(diff(dir))
  
  badness = factor_acc *max(acc)^2 +
            factor_dist*sum(ds)^2 + 
            factor_cdir*max(ddir)^2
  
  return(badness)
}

```

```{r}

x_seq <- seq(0, 10, 0.1)
y_seq <- seq(0, 10, 0.1)
df_1a <- data.frame(
  x   = rep(x_seq, times = length(y_seq)),
  y   = rep(y_seq, each  = length(x_seq)),
  rho = NA
)
for(k in seq_len(nrow(df_1a))) {
  p          <- make_bezier_path_1free(x = c(df_1a$x[k], df_1a$y[k]))
  df_1a$rho[k] <- get_path_badness(p)
}
head(df_1a)
tail(df_1a)
```

b) (2 marks) Draw a contour plot of the response surface, where the response is 'badness', and the surface is on the x and y coordinates of the second guide point. Use the following code as an example. (Hint: You can convert `df$rho` into a `matrix()` of 101 rows and columns)

```{r}
# x is a 21-length vector
# y is a 21-length vector
# z is x^2 + y^2. z is a 21x21 matrix
x <- -10:10
y <- -10:10
z <- sqrt(outer(x ^ 2, y ^ 2, "+"))

contour(x, y, z)
```

```{r}

zmat <- matrix(df_1a$rho, nrow = length(x_seq), byrow = FALSE)
contour(x_seq, y_seq, zmat)
```

c) (3 marks) Fit a third order polynomial with interactions to the response surface.

Namely, use the linear least squares method to find the coefficient estimates $\mathbf{\hat\beta}$ to, the following formula.

$$\rho = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3 + \beta_4y + \beta_5y^2 + \beta_6y^3 + \beta_7xy + \beta_8x^2y + \beta_9xy^2 + \epsilon$$

$$\hat\beta = (X^TX)^{-1}X^TY$$ 


Where $\omega$ is the estimated badness, where X is `cbind()` of `1, df_1a$x, ...`. This should be $10$ columns (one for each beta value) and $101^2$ rows (one for each coordinate), and where Y is the vector of $101^2$ badness values.

You may use `lm()`, specifically `summary(lm())$coef` to check your work, but your code should include the matrix multiplications of X and Y. Hints: `solve(X)` gives the inverse of X, `t(X)` gives the transpose, and `X %*% Y gives the matrix multiplication of X and Y`.

```{r}
X        <- cbind(
  1,
  df_1a$x,
  df_1a$x^2,
  df_1a$x^3,
  df_1a$y,
  df_1a$y^2,
  df_1a$y^3,
  df_1a$x * df_1a$y,
  df_1a$x^2 * df_1a$y,
  df_1a$x * df_1a$y^2
)
Y        <- df_1a$rho
beta_hat <- solve(t(X) %*% X, t(X) %*% Y)
beta_hat

summary(lm(rho ~ poly(x,3) + poly(y,3) + I(x*y) + I(x^2*y) + I(x*y^2), data = df_1a))

```


d) (2 marks) Write a function named `rho_fn` which takes in `x=c(x0, y0)`, where `x0` and `y0` are the coordinates of the first guide point in the Bezier curve. `rho_fn` should do two things:

- Generate the path object from `x` by calling a function.
- Calculate $\rho$, the badness of that path, by calling function.
- Return $\rho$.

```{r}
rho_fn <- function(x) {
  p <- make_bezier_path_1free(x = x)
  get_path_badness(p)
}
```
e) (1 mark) Use `optim( ...,method="L-BFGS-B", lower=c(0,0), upper=(10,10))` and the `rho_fn` function you made in part d to find 


$$\arg\min_{0\leq x\leq 10, 0 \leq y \leq 10} \rho(x,y)$$
```{r}
res1 <- optim(
  par    = c(3, 5),
  fn     = rho_fn,
  method = "L-BFGS-B",
  lower  = c(0, 0),
  upper  = c(10, 10)
)
res1$par




```

To check if y=0 truly minimize $$\rho$$:
```{r}

x0   <- res1$par[1]
eps <- 1e-3
xs  <- seq(max(0, x0-0.5), min(10, x0+0.5), length=200)
rho_y0   <- sapply(xs, function(xx) rho_fn(c(xx,    0)))
rho_yeps <- sapply(xs, function(xx) rho_fn(c(xx, eps)))
plot(xs, rho_y0,   type="l", ylim=range(c(rho_y0, rho_yeps)),
     xlab="x", ylab="ρ", main="Slice at y=0 vs y=ε")
lines(xs, rho_yeps, col="red")
abline(v = x0, lty=2)
```

Since the black curve lies at or below the red curve, it confirms that y=0 truly minimizes $$\rho$$


f) (2 marks) Extend the function `make_bezier_path_1free` into `make_bezier_path_2free`, in which `x` takes in 4 values, the first two being x and y of the first guide point, and the second two being x and y of the second guide point, respectively.
```{r}
make_bezier_path_2free <- function(x, begin = c(0, 0), end = c(10, 2)) {
  px <- c(begin[1], x[1], x[3], end[1])
  py <- c(begin[2], x[2], x[4], end[2])
  t  <- seq(0, 1, 0.01)
  list(
    x = (1 - t)^3 * px[1] +
        3 * (1 - t)^2 * t * px[2] +
        3 * (1 - t) * t^2 * px[3] +
        t^3 * px[4],
    y = (1 - t)^3 * py[1] +
        3 * (1 - t)^2 * t * py[2] +
        3 * (1 - t) * t^2 * py[3] +
        t^3 * py[4]
  )
}
```

g) (2 marks) Make a new `rho_fn2` based on `rho_fn`, but for `make_bezier_path_2free`, and use `optim` with it to find and report the best location to put both guide points.

```{r}
rho_fn2 <- function(x) {
  p <- make_bezier_path_2free(x = x)
  get_path_badness(p)
}
res2 <- optim(
  par    = c(3, 5, 7, 5),
  fn     = rho_fn2,
  method = "L-BFGS-B",
  lower  = c(0, 0, 0, 0),
  upper  = c(10, 10, 10, 10)
)
res2$par
```


\newpage


## QUESTION 3 (16 marks): Quantile Regression


Part 0) Either run the following code to get the census data on every subdivision in Canada, or use the dataset in Learn, if you're having issues with your API.

```{r, eval=FALSE}
library(cancensus)

#all_vecs = list_census_vectors("CA21")
# Total English         v_CA21_2209
# Total French          v_CA21_2212
# Full time Full year   v_CA21_6525
# Average wks worked    v_CA21_6531
# Median after-tax inc  v_CA21_986
# Median Total inc      v_CA21_983


all_regions = list_census_regions("CA21") # 2021 census
csd_all = list(CSD = all_regions$region)
all_regions = data.frame(all_regions)

thislevel = "CSD"
census_data = get_census(dataset = "CA21",
                         regions = csd_all,
                         vectors = c("v_CA21_2209", 
                                     "v_CA21_2212", 
                                     "v_CA21_983", 
                                     "v_CA21_986", 
                                     "v_CA21_6525", 
                                     "v_CA21_6531"),
                         level = thislevel)

names(census_data)[ncol(census_data) - 5 ] = "English_Speakers"
names(census_data)[ncol(census_data) - 4 ] = "French_Speakers"
names(census_data)[ncol(census_data) - 3 ] = "Median_Before_Tax"
names(census_data)[ncol(census_data) - 2 ] = "Median_After_Tax"
names(census_data)[ncol(census_data) - 1 ] = "FTime_FYear_Workers"
names(census_data)[ncol(census_data) - 0 ] = "Avg_Weeks_Worked"


census_data = data.frame(census_data)
census_data = subset(census_data, Population > 0 & !is.na(Avg_Weeks_Worked) 
                     & !is.na(Median_After_Tax))
filename = paste0("Stat341_S25_A2_Census_Data_",thislevel,".csv")
write.csv(census_data, filename, row.names=FALSE)
```


a) (5 marks) Create a function called `rho_lm_fn(x, data)`, where x is a 2x1 vector of the intercept (alpha) and slope (beta) respectively, which returns the sum of squares of the residuals.

```{r}
rho_lm_fn <- function(x, data) {
  alpha <- x[1]
  beta  <- x[2]
  resid <- data$Median_After_Tax - (alpha + beta * data$Avg_Weeks_Worked)
  sum(resid^2)
}


```

b) (2 marks) Use the `optim()` function to find the alpha and beta of the least squares solution. Use `lm()$coef` and compare your results.
```{r}
lm_coef <- lm(Median_After_Tax ~ Avg_Weeks_Worked, data = census_data)$coef
res_lm_opt <- optim(
  par    = lm_coef,
  fn     = rho_lm_fn,
  data   = census_data,
  method = "BFGS"
)
res_lm <- res_lm_opt

round(res_lm_opt$par, 6)
round(lm_coef, 6)
```

c) (3 marks) Create a function called `rho_qr_fn(x, tau, data)`, where x is a 2x1 vector of intercept (alpha) and slope (beta) respectively, which returns the sum of the generalized absolute value loss where `tau` is the target quantile.


```{r}
rho_qr_fn <- function(x, tau, data) {
  alpha <- x[1]
  beta  <- x[2]
  resid <- data$Median_After_Tax - (alpha + beta * data$Avg_Weeks_Worked)
  sum(resid * (resid >= 0) * tau + resid * (resid < 0) * (tau - 1))
}
```



d) (4 marks) Use the `optim()` function to find the alpha and beta of the generalized absolute loss solutions for $\tau = 0.1, 0.25, 0.5, 0.75, 0.9$. Use `rq()$coef` function in to the `quantreg` package to check your work.
```{r}
library(quantreg)
taus    <- c(0.1, 0.25, 0.5, 0.75, 0.9)
res_qr  <- sapply(taus, function(tau) {
  coef <- optim(par = res_lm$par, fn = rho_qr_fn, tau = tau, data = census_data)$par
  setNames(coef, c("alpha","beta"))
})
rq_coef <- sapply(taus, function(tau) {
  coef(rq(Median_After_Tax ~ Avg_Weeks_Worked, tau = tau, data = census_data))
})
colnames(res_qr)  <- paste0("tau=", taus)
colnames(rq_coef) <- paste0("tau=", taus)
res_qr
rq_coef

```
e) (2 marks)  Compare your results to the `rq` results, do they all agree? Do they mostly agree? What does this imply about the stability of a generalized optimizer like `optim` compared to an application-specific optimizer like `rq` or `lm`?

```{r}
all.equal(res_qr, rq_coef, tolerance = 1e-6)

```

Do they all agree: Yes. To within numerical rounding at the 4th to 6th decimal place they are identical.

Implication for stability: A purpose-built solver like rq() (or lm()) uses highly tuned algorithms that guarantee convergence to the exact solution for its loss function. A general optimizer like optim() can also recover the same answer, but it may require a gradient‐based method like BFGS rather than Nelder–Mead and sufficiently tight convergence tolerances.
