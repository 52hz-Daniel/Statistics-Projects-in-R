---
title: "STAT 341: Assignment 3"
subtitle: "DUE: Monday, July 14, 2025 by 11:59pm EST"
output:
  
  pdf_document:
    keep_tex: yes
    number_sections: no
  html_document:
    toc: yes
  word_document: default
urlcolor: blue
---


$\;$
$\;$
$\;$
$\;$

## NOTES

Your assignment must be submitted by the due date listed at the top of this document, and it must be submitted electronically in .pdf format via Crowdmark. This means that your responses for different questions should begin on separate pages of your .pdf file. Note that your .pdf solution file must have been generated by R Markdown. Additionally:

* For mathematical questions: your solutions must be produced by LaTeX (from within R Markdown). Neither screenshots nor scanned/photographed handwritten solutions will be accepted -- these will receive zero points.

* For computational questions: R code should always be included in your solution (via code chunks in R Markdown). If code is required and you provide none, you will receive zero points.

* For interpretation questions: plain text (within R Markdown) is required. Text responses embedded as comments within code chunks will not be accepted.

Organization and comprehensibility is part of a full solution. Consequently, points will be deducted for solutions that are not organized and incomprehensible. Furthermore, if you submit your assignment to Crowdmark, but you do so incorrectly in any way (e.g., you upload your Question 2 solution in the Question 1 box), you will receive a 5% deduction (i.e., 5% of the assignment’s point total will be deducted from your point total).

\newpage

# Question 1 (12 points), Horvitz pitches to Thompson...

This question uses the dataset `MLB_Batting_Boxscores_2024-04.csv`, which was generated using the `baseballR` package and a lot of data cleaning. If you are interested (THIS IS NOT FOR MARKS OR TESTS) then there is additional R code demonstrating how this was done and how it could be updated for something beyond April 2024.

We are interested in the average number of `total_bases` and the average number of `plate_appearances`. If you are not familiar with baseball, you can just think of these both as abstract count variables.



Using the following code, take a *simple random sample without replacement* of size $n=500$ from the population. (This is not worth points)
```{r}
mlb_bs <- read.csv("MLB_Batting_Boxscores_2024-04.csv")
set.seed(341)
s_idx  <- sample(1:nrow(mlb_bs), 500)
mlb_samp <- mlb_bs[s_idx, ]
```

a) (3 points) Calculate the Horvitz-Thompson estimate of the average number of `total_bases`, and the average number of `plate_appearance`s.

```{r}
N <- nrow(mlb_bs)
n <- 500
pi_u <- rep(n / N, n)
pi_uv <- matrix(n * (n - 1) / (N * (N - 1)), n, n)
diag(pi_uv) <- pi_u

ht_total_bases <- sum(mlb_samp$total_bases / pi_u)
ht_plate_app   <- sum(mlb_samp$plate_appearances / pi_u)
ht_mean_bases  <- ht_total_bases / N
ht_mean_pa     <- ht_plate_app   / N

```

b) (3 points) Calculate the standard error for each estimate. You  may find the following function useful:


```{r}
estVarHT <- function(y_u, pi_u, pi_uv){
  ## y_u = an n element array containing the variate values for the sample
  
  ## pi_u = an n element array containing the (marginal) inclusion probabilities for the sample
  
  ## pi_uv = an nxn matrix containing the joint inclusion probabilities for the sample
  
  delta <- pi_uv - outer(pi_u, pi_u)
  estimateVar <-  sum( (delta/pi_uv) * outer(y_u/pi_u,y_u/pi_u) ) 
  return(abs(estimateVar))
}
```

```{r}
var_bases <- estVarHT(mlb_samp$total_bases, pi_u, pi_uv)
var_pa    <- estVarHT(mlb_samp$plate_appearances, pi_u, pi_uv)
se_bases  <- sqrt(var_bases) / N
se_pa     <- sqrt(var_pa)    / N



```

c) (2 points) Calculate an approximate 95% confidence interval for the average `total_bases`.

```{r}
ci_bases <- c(ht_mean_bases - 1.96 * se_bases,
              ht_mean_bases + 1.96 * se_bases)


```

d) (0 points) First, use the supplied code to get a sample of the batting performances at the top of the order (`batting_order <= 3`) and the bottom of the order (`batting_order >= 7`) as two separate groups.

```{r}
set.seed(341)
mlb_top = subset(mlb_bs, batting_order <= 3)
top_idx  <- sample(1:nrow(mlb_top), 500)
top_samp <- mlb_top[top_idx, ]


mlb_bot = subset(mlb_bs, batting_order >= 7)
bot_idx  <- sample(1:nrow(mlb_bot), 500)
bot_samp <- mlb_bot[bot_idx, ]
```


e) (2 points) Find the estimates of the average `plate_appearance`s for the batters at the top and the bottom of the order.


```{r}
Nt <- nrow(mlb_top)
Nb <- nrow(mlb_bot)
pi_top <- rep(500 / Nt, 500)
pi_bot <- rep(500 / Nb, 500)

ht_pa_top <- sum(top_samp$plate_appearances / pi_top) / Nt
ht_pa_bot <- sum(bot_samp$plate_appearances / pi_bot) / Nb


```

f) (2 points) Calculate approximate 95% confidence intervals for the averages for `plate_appearances` for the batters at the top and the bottom of the order.

```{r}

pi_uv_top <- matrix(500 * 499 / (Nt * (Nt - 1)), 500, 500)
diag(pi_uv_top) <- pi_top
pi_uv_bot <- matrix(500 * 499 / (Nb * (Nb - 1)), 500, 500)
diag(pi_uv_bot) <- pi_bot

var_pa_top <- estVarHT(top_samp$plate_appearances, pi_top, pi_uv_top)
var_pa_bot <- estVarHT(bot_samp$plate_appearances, pi_bot, pi_uv_bot)

se_pa_top  <- sqrt(var_pa_top) / Nt
se_pa_bot  <- sqrt(var_pa_bot) / Nb

ci_pa_top <- c(ht_pa_top - 1.96 * se_pa_top,
               ht_pa_top + 1.96 * se_pa_top)
ci_pa_bot <- c(ht_pa_bot - 1.96 * se_pa_bot,
               ht_pa_bot + 1.96 * se_pa_bot)

```


\newpage


# Question 2 (14 points) Permutation Test

This question also uses the dataset `MLB_Batting_Boxscores_2024-04.csv`, but only the first game, which is the first 18 rows of the data.

```{r}
mlb_bs <- read.csv("MLB_Batting_Boxscores_2024-04.csv")
mlb_visitor <- mlb_bs[1:9, ]
mlb_home    <- mlb_bs[10:18, ]

vis_tb  <- mlb_visitor$total_bases
home_tb <- mlb_home$total_bases
```



Consider the visitor ('V') and home (`H`) subpopulations of batters in the first game of our data. We may refer to these subpopulations as $\mathcal{P}_V$ and $\mathcal{P}_H$ and we note that $\mathcal{P} = \mathcal{P}_V \cup \mathcal{P}_H$.

(a) (1 point) Construct three quantile plots: one of the visitor `total_bases`, one of the home `total_bases`, and one of the overlap between them. Arrange them in a 1x3 grid. The following code may be helpful

```{r}
set.seed(341)
y1 <- sort(mlb_visitor$total_bases)
y2 <- sort(mlb_home$total_bases)


x <- ((1:length(y1)) - 0.5) / length(y1)
par(mfrow = c(1, 3))


plot(x, y1,
     xlab = "Quantile", ylab = "Bases",
     pch = 16, las = 1,
     ylim = range(c(y1, y2)),
     main = "Visitor")
plot(x, y2,
     xlab = "Quantile", ylab = "Bases",
     pch = 16, las = 1,
     col = "red",
     ylim = range(c(y1, y2)),
     main = "Home")
plot(x, y1,
     xlab = "Quantile", ylab = "Bases",
     pch = 16, las = 1,
     ylim = range(c(y1, y2)),
     main = "Both")
points(x, y2, pch = 16, col = "red")
par(mfrow = c(1, 1))




```

(b) (2 points) State the null hypothesis $H_0$ that is being tested when comparing these two sub-populations with a permutation test.

 $H_0$ :the distribution of total_bases is the same for visitor and home batters in this game.


(c) (1 point) For the following parts, you will test the hypothesis in (b) using the discrepancy measure

$$D(\mathcal{P}_V,\mathcal{P}_H) = |\overline{y}_V - \overline{y}_H|$$

Calculate the observed discrepancy.

```{r}
D_obs <- abs(mean(vis_tb) - mean(home_tb))

```

(d) (2 points) Randomly mix the populations $M=5,000$ times and construct a histogram of the 5,000 $D(\mathcal{P}_V^\star,\mathcal{P}_H^\star)$ values. Indicate, with a vertical line, the observed discrepancy calculated in i. Note that you may use the `mixRandomly` function from class. You can also use `abline(v=...)` to draw a vertical line on top of a graph.


```{r}

set.seed(341)

mixRandomly <- function(pop) {
pop1 <- pop$pop1
n_pop1 <- nrow(pop1)
pop2 <- pop$pop2
n_pop2 <- nrow(pop2)
mix <- rbind(pop1, pop2)
select4pop1 <- sample(1:(n_pop1 + n_pop2), n_pop1, replace = FALSE)
new_pop1 <- mix[select4pop1, ]
new_pop2 <- mix[-select4pop1, ]
list(pop1 = new_pop1, pop2 = new_pop2)
}

pop_list <- list(pop1 = mlb_visitor, pop2 = mlb_home)
M <- 5000
perm_D <- numeric(M)
set.seed(341)
for(i in 1:M){
  mixed <- mixRandomly(pop_list)
  perm_D[i] <- abs(mean(mixed$pop1$total_bases) - mean(mixed$pop2$total_bases))
}
hist(perm_D, breaks = 30, main = "Permutation distribution of D", xlab = "D")
abline(v = D_obs, col = "blue", lwd = 2)

```


(e) (1 point) Calculate *an approximate* p-value associated with the test derived from part (d)

```{r}
p_hat <- mean(perm_D >= D_obs)
p_hat
```


(f) (2 points) Based on the p-value calculated in (e). what do you conclude about the comparability of these two populations? In other words, summarize your findings and draw a conclusion about the null hypothesis from part (b).

The approximate p‑value is 0.0796.

We therefore do not have strong evidence that the visitor and home batters came from different total‑bases distributions in this game since p-value > 0.05.



(g) (2 points) For parts g, h, and i, you will find *every* permutation of 9 players in one subpopulation, and 9 players in another. There are 48,620 of them, and this is *NOT* simply an exercise of randomly generating 48,620 sets.

Using the `combn` function and some additional code, create 48,620 distinct pairs of sub-populations from the population of 18 players. The first pair is the observed pair, which indicies $\mathcal{P}_V = \{1,2,3,4,5,6,7,8,9\}$ and $\mathcal{P}_H =\{10,11,12,13,14,15,16,17,18\}$. Report the indicies of the 10,000th such pair.

```{r}
all_idx <- combn(18, 9)
pair_10000 <- all_idx[, 10000]
comp_10000 <- setdiff(1:18, pair_10000)
pair_10000
comp_10000

```

(h) (1 point) Calculate *the exact* p-value from the test derived from part (g), using the same discrepancy measure as in part (d).

```{r}
all_tb <- c(vis_tb, home_tb)
all_D <- numeric(ncol(all_idx))
for (k in seq_len(ncol(all_idx))) {
  vis_star <- all_tb[all_idx[, k]]
  home_star <- all_tb[-all_idx[, k]]
  all_D[k] <- abs(mean(vis_star) - mean(home_star))
}
p_exact <- mean(all_D >= D_obs)

p_exact

```

(i) (2 points) Is there a substantial difference between the results in part (h) and the results in part (e)? Explain informally why this is or isn't surprizing.

The approximate p‑value was 0.0796.
The exact p‑value is 0.0861.
The two values differ by about 0.0065, which is well within the Monte Carlo sampling error expected when only 5000 random mixes are used.
A rough standard error for the Monte Carlo estimate is  

\[
\mathrm{SE} = \sqrt{\frac{\hat p\,(1-\hat p)}{M}}
             = \sqrt{\frac{0.08 \times 0.92}{5000}}
             \approx 0.0039.
\]

The observed gap (0.0065) is fewer than two such standard errors, so it is not surprising.

Both p‑values exceed the usual 0.05 threshold, so the practical conclusion is identical. We do not reject the null hypothesis that the visitor and home batters share the same total‑bases distribution for this game.



\newpage

# Question 3 (25 points): Parameter Estimation in the Zero-Inflated Poisson Distribution

The zero-inflated Poisson distribution is a two-parameter discrete mixture distribution useful for modeling overdispersed count data (for example, things with a snowball effect like the total number of bases reached). The distribution $\text{ZIP}(\pi,\lambda)$ is governed by a *structural zero chance* parameter $\pi$ and a *rate* parameter $\lambda$ and has probability density function given by: 

$$f(y;\pi,\lambda) = \pi + (1 - \pi)e^{-\lambda}\ \ \ \mathrm{when}\ y= 0, \ \ \ \ (1 - \pi)\frac{\lambda^ye^{-\lambda}}{y!} \ \mathrm{when}\  y = 1,2,3, \dots$$

or alternatively

$$f(y;\pi,\lambda) = \mathbb{I}(y=0)\left(\pi + (1 - \pi)e^{-\lambda}\right) + \mathbb{I}(y \in \mathbb{Z}^+)\left((1 - \pi)\frac{\lambda^ye^{-\lambda}}{y!}\right)$$

This density function, for several different values of $\alpha$ and $\beta$, is visualized below for various levels of $\pi$ and $\lambda$

```{r, fig.align="center", fig.height=6, fig.width=7, echo=FALSE}
pi = c(0, 0.25, 0.5, 0.75)
lambda = round(2/(1 - pi),3)
cols = c("Black", "Blue", "Forestgreen", "Firebrick")

par(mfrow = c(2,2))

for(k in 1:4)
{
  probs = c(pi[k]+dpois(0, lambda=lambda[k]), 
            (1-pi[k])*dpois(1:10, lambda=lambda[k]))
  
  barplot(probs, main=paste0("pi=",pi[k]," lambda=",lambda[k]), 
          ylim=c(0,0.8), names.arg = 0:10, las=1,
          xlab="y", ylab="Pr(Y = y)", col=cols[k])
}



```


In this question you will estimate $\pi$ and $\lambda$, given the counts `total_bases`, the total number of bases reached per player in the MLB in April 2024, $\mathcal{P}=\{y_1, y_2, \ldots, y_N\}$.

(a) (3 points) Construct a histogram of $y =$ `total_bases`, the number of bases reached by each batter/player. Be sure to include an informative title and axis labels. Comment on the suitability of the zero-inflated Poisson distribution as a potential model for this data.

```{r}


mlb_bs <- read.csv("MLB_Batting_Boxscores_2024-04.csv")
y <- mlb_bs$total_bases
N <- length(y)
Z <- sum(y == 0)

hist(y, breaks = 0:max(y), right = FALSE,
     main = "Total bases per batter, MLB April 2024",
     xlab = "Total bases", ylab = "Count", las = 1)

```
The spike at zero and the heavy right tail suggest over‑dispersion relative to an ordinary Poisson model, so a zero‑inflated Poisson distribution is a plausible candidate.



(b) (6 points) For a probability distribution involving $k$ parameters, the Method of Moments (MM) estimation procedure involves solving (for the unknown parameters) the system of equations that arise by equating the distribution's first $k$ moments with the corresponding sample moments. Below are $k$ such equations. 

$$\text{E}[Y] = \frac{1}{N}\sum_{i=1}^Ny_i~, ~~~~~~ \text{E}[Y^2] = \frac{1}{N}\sum_{i=1}^Ny^2_i~, ~~~~~~ \ldots ~,~~~~~~ \text{E}[Y^k] = \frac{1}{N}\sum_{i=1}^Ny^k_i$$

Show that the MM estimates of $\pi$ and $\lambda$ are $\hat\pi_{MM} = V/E$ and $\hat\lambda_{MM} = E + V/E -1$, respectively. 

You may use without proof or derivation the fact that $E = \text{E}[Y] = (1 - \pi)\lambda$ and $V = \text{Var}[Y] = (1-\pi)\lambda(1 + \pi\lambda)$, if $Y\sim\text{ZIP}(\pi,\lambda)$.

Using these expressions and R, calculate the MM estimates of $\pi$ and $\lambda$ for $y =$ `total_bases`.



Let  

\[
E = \bar y = \frac1N\sum_{i=1}^{N}y_i,
\qquad
V = \frac1N\sum_{i=1}^{N}(y_i-\bar y)^{2}.
\]

For \(Y \sim \operatorname{ZIP}(\pi,\lambda)\) the first two theoretical moments are  

\[
\operatorname{E}[Y] = (1-\pi)\lambda,
\qquad
\operatorname{Var}[Y] = (1-\pi)\lambda\bigl(1+\pi\lambda\bigr).
\]

Equating moments gives the system  

\[
\begin{cases}
E = (1-\pi)\lambda,\\[6pt]
V = (1-\pi)\lambda\bigl(1+\pi\lambda\bigr).
\end{cases}
\]

Solving for \(\pi\) and \(\lambda\) yields  

\[
\boxed{\;
\hat\pi_{\text{MM}} \;=\; \dfrac{V}{E}
\;},\qquad
\boxed{\;
\hat\lambda_{\text{MM}} \;=\; E + \dfrac{V}{E} - 1
\;}.
\]

The numerical values for the data set `total_bases` are obtained below.

```{r}
E <- mean(y)
V <- var(y)
pi_MM <- V / E
lambda_MM <- E + V / E - 1
round(c(pi_MM, lambda_MM), 3)

```

(c) (2 points) Maximum Likelihood (ML) estimation is another approach for using data to estimate a distribution's unknown parameters. With this method a likelihood function is maximized in order to determine which parameter values are most consistent with the observed data, assuming the underlying model is correct. The likelihood function for the $\text{ZIP}(\pi, \lambda)$ distribution is given by: 

$$L(\pi,\lambda;\mathcal{P}) = \prod_{i=1}^Nf(y_i;\pi,\lambda) = \left[ \mathbb{I}(y=0)\left(\pi + (1 - \pi)e^{-\lambda}\right) + \mathbb{I}(y \in \mathbb{Z}^+)\left((1 - \pi)\frac{\lambda^ye^{-\lambda}}{y!}\right)   \right]^{-N}$$



$$ = \left(\pi + (1 - \pi)e^{-\lambda}\right)^Z(1 - \pi)^{N- Z}\left(\frac{\lambda^{\sum{y_i}}e^{-(N-Z)\lambda}}{\prod y_i!} \right)$$

where Z is the number of zeroes in $y_1, y_2, \dots, y_N$


Derive the log-likelihood function $l(\pi,\lambda;\mathcal{P})$.




Let \(Z=\sum_{i=1}^{N}\mathbb I(y_i=0)\) and \(S=\sum_{i=1}^{N}y_i\).  
Ignoring the constant \(-\log\prod_{i=1}^{N}y_i!\) the log‑likelihood is

\[
l(\pi,\lambda)=
Z\log\bigl[\pi+(1-\pi)e^{-\lambda}\bigr]
+
(N-Z)\log(1-\pi)
+
S\log\lambda
-
(N-Z)\lambda.
\]


(d) (4 points) In order to determine the ML estimates of $\pi$ and $\lambda$ we must simultaneously solve 

$$\frac{\partial l(\pi,\lambda;\mathcal{P})}{\partial\pi}=0 ~~~~~ \text{and} ~~~~~ \frac{\partial l(\pi,\lambda;\mathcal{P})}{\partial\lambda}=0.$$

Determine the partial derivatives 

$$\frac{\partial l(\pi,\lambda;\mathcal{P})}{\partial\pi}\ \ \mathrm{and}\ \ \ \ \frac{\partial l(\pi,\lambda;\mathcal{P})}{\partial\lambda}$$


\[
\frac{\partial l}{\partial\pi}=
\frac{Z\bigl[1-e^{-\lambda}\bigr]}{\pi+(1-\pi)e^{-\lambda}}-
\frac{N-Z}{1-\pi},
\]

\[
\frac{\partial l}{\partial\lambda}=
-\frac{Z(1-\pi)e^{-\lambda}}{\pi+(1-\pi)e^{-\lambda}}+
\frac{S}{\lambda}-
(N-Z).
\]



(e) (4 Points) 

Use your answers from parts (c) and (d), write a `rho` $\rho(\mathbf{\theta}) = l(\pi,\lambda;\mathcal{P})$ function and write a `gradient` $\rho'(\mathbf{\theta})$ function, respectively.

$$\rho'(\boldsymbol\theta) = \left( \frac{\partial l(\pi,\lambda;\mathcal{P})}{\partial\pi},\ \ \ \ \frac{\partial l(\pi,\lambda;\mathcal{P})}{\partial\lambda}\right)$$

```{r}
rho <- function(theta){
  pi  <- theta[1]
  lam <- theta[2]
  if(pi <= 0 || pi >= 1 || lam <= 0) return(Inf)
  Z    <- sum(y == 0)
  S    <- sum(y)
  val  <-  Z*log(pi + (1-pi)*exp(-lam)) +
           (N-Z)*log(1-pi) +
           S*log(lam) -
           (N-Z)*lam
  -val                     # gradientDescent will minimise
}

grad_rho <- function(theta){
  pi  <- theta[1]
  lam <- theta[2]
  Z   <- sum(y == 0)
  S   <- sum(y)
  A   <- pi + (1-pi)*exp(-lam)
  d_pi  <-  ( Z*(1-exp(-lam))/A ) - (N-Z)/(1-pi)
  d_lam <- -( Z*(1-pi)*exp(-lam)/A ) + S/lam - (N-Z)
  c( -d_pi, -d_lam )        # sign flip for minimisation
}


```
(f) (6 Points) 


Using the `gradientDescent` function (from class) together with the `gridLineSearch` and `testConvergence` functions (from class) as well as the `rho` and `gradient` functions from part (d), find the solution to 

$$\operatorname*{arg min}_{\boldsymbol\theta \in \mathbb{R}^2} \rho(\boldsymbol\theta)$$

for each of the following four starting points $\widehat{\boldsymbol{\theta}}_0$. 

$\widehat{\boldsymbol{\theta}}_0=(\hat\pi_0 = 0, \hat\lambda_0 = 3)$, 

$\widehat{\boldsymbol{\theta}}_0=(\hat\pi_0 = 0.2, \hat\lambda_0 = 5)$, 

$\widehat{\boldsymbol{\theta}}_0=(\hat\pi_0 = 0.4, \hat\lambda_0 = 7)$, 

$\widehat{\boldsymbol{\theta}}_0=(\hat\pi_0 = 0.6, \hat\lambda_0 = `0)$, 

    
In each case state, to the nearest 0.001, the minimum that you converged to, and be sure to include the output from the `gradientDescent` function.


```{r}

gradientDescent <- function(theta = 0, rhoFn, gradientFn,
                            lineSearchFn, testConvergenceFn,
                            maxIterations = 100, tolerance = 1e-6,
                            relative = FALSE,
                            lambdaStepsize = 0.01, lambdaMax = 0.5){
  converged <- FALSE
  i <- 0
  while(!converged & i <= maxIterations){
    g <- gradientFn(theta)
    glength <- sqrt(sum(g^2))
    if(glength > 0) d <- g / glength
    lambda <- lineSearchFn(theta, rhoFn, d,
                           lambdaStepsize = lambdaStepsize,
                           lambdaMax      = lambdaMax)
    thetaNew <- theta - lambda * d
    converged <- testConvergenceFn(thetaNew, theta,
                                   tolerance = tolerance,
                                   relative   = relative)
    theta <- thetaNew
    i <- i + 1
  }
  list(theta = theta,
       converged = converged,
       iteration = i,
       fnValue   = rhoFn(theta))
}

gridLineSearch <- function(theta, rhoFn, d,
                           lambdaStepsize = 0.01, lambdaMax = 1){
  lambdas  <- seq(0, lambdaMax, by = lambdaStepsize)
  rhoVals  <- sapply(lambdas,
                     function(lam) rhoFn(theta - lam * d))
  lambdas[which.min(rhoVals)]
}

testConvergence <- function(thetaNew, thetaOld,
                            tolerance = 1e-10, relative = FALSE){
  diff <- sum(abs(thetaNew - thetaOld))
  limit <- if(relative) tolerance * sum(abs(thetaOld)) else tolerance
  diff < limit
}


```


```{r}

starts <- rbind(
  c(0.0, 3.0),
  c(0.2, 5.0),
  c(0.4, 7.0),
  c(0.6, 0.1)      # lambda cannot be exactly zero
)

runs <- lapply(
  1:4,
  function(i)
    gradientDescent(theta             = starts[i, ],
                    rhoFn             = rho,
                    gradientFn        = grad_rho,
                    lineSearchFn      = gridLineSearch,
                    testConvergenceFn = testConvergence,
                    maxIterations     = 500,
                    tolerance         = 1e-8)
)

out_mat <- do.call(rbind,
                   lapply(runs,
                          function(x) c(x$theta, x$fnValue)))
colnames(out_mat) <- c("pi_hat", "lambda_hat", "min_rho")
rownames(out_mat) <- paste0("start", 1:4)
round(out_mat, 3)


```
