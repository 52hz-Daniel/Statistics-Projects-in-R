---
title: "STAT 341: Assignment 1"
subtitle: "DUE: Friday, May 23, 2025 by 11:59pm EST"
output:
  pdf_document:
    keep_tex: yes
    number_sections: no
  html_document:
    toc: yes
  word_document: default
urlcolor: blue
---


$\;$
$\;$
$\;$
$\;$

## NOTES

Your assignment must be submitted by the due date listed at the top of this document, and it must be submitted electronically in .pdf format via Crowdmark. This means that your responses for different questions should begin on separate pages of your .pdf file. Note that your .pdf solution file must have been generated by R Markdown. Additionally:

* For mathematical questions: your solutions must be produced by LaTeX (from within R Markdown). Neither screenshots nor scanned/photographed handwritten solutions will be accepted -- these will receive zero points.

* For computational questions: R code should always be included in your solution (via code chunks in R Markdown). If code is required and you provide none, you will receive zero points.

* For interpretation questions: plain text (within R Markdown) is required. Text responses embedded as comments within code chunks will not be accepted.

Organization and comprehensibility is part of a full solution. Consequently, points will be deducted for solutions that are not organized and incomprehensible. Furthermore, if you submit your assignment to Crowdmark, but you do so incorrectly in any way (e.g., you upload your Question 2 solution in the Question 1 box), you will receive a 5% deduction (i.e., 5% of the assignment’s point total will be deducted from your point total).

\newpage

## QUESTION 1 (18 marks): Dog Agility Part 1 - User-defined functions

This is part 1 of a project that will span 3 or 4 assignments in which the end goal is to analyze maps of dog agility courses and assess them for things like overall difficulty. In this first part, we're just going to be establishing the functions necessary to determine the physical dynamics of an arbitrary path.



![](BMcDonald-PrSTD-2021-01-02.jpg)


A lot of what we're going to do in Stat 341 involves user-defined functions, so it's good to get used to making and using them here.



The paths we'll be using are generated from the following:

- A cubic Bezier curve (`bezier_path_x` and `bezier_path_y`)
- A spaceship leaving one body and landing on another. (`space_path`)
- A heart (`heart_path`)

```{r}
px = c(0,3,7,10)
py = c(0,5,5,2)
t = seq(from=0, to=1, by=0.01)
bezier_path_x = (1-t)^3*px[1] + 
                3*(1-t)^2*t*px[2] +  
                3*(1-t)*t^2*px[3] +  
                t^3*px[4]
bezier_path_y = (1-t)^3*py[1] + 
                3*(1-t)^2*t*py[2] +  
                3*(1-t)*t^2*py[3] +  
                t^3*py[4]


t = seq(from=0, to=2*pi, length=101)
space_path_x = c((t + 4)*sin(3*t), 
                 seq(from=0, to=30, length=40), 
                 rev(30 - (t + 1)*sin(3*t)))
space_path_y = c((t + 4)*cos(3*t) , 
                 seq(from=10.283, to=19.283, length=40),
                 rev(12 + (t + 1)*cos(3*t)))


t = seq(from=0, to=2*pi, length=101)
heart_path_x = 16*sin(t)^3
heart_path_y = 13*cos(t) - 5*cos(2*t) - 2*cos(3*t) - cos(4*t)



par(mfrow = c(1,3))
plot(bezier_path_x, bezier_path_y, type="b", main="Cubic Bezier path")
plot(space_path_x, space_path_y, type="b", main="Space orbital path")
plot(heart_path_x, heart_path_y, type="b", main="Heart path")
par(mfrow = c(1,1))

  
```


Here are an example function to get you started. This measures the mean velocity along a path of time snapshots in 2D space. (Alternatively, replacing mean)

```{r}
mean_velocity_2d = function(x,y)
{
  dx = diff(x) # or x[-1] - x[-n]
  dy = diff(y)
  v = sqrt(dx^2 + dy^2)
  m_v = mean(v)
  return(m_v)
}
```


Create and report functions for the following.

a) (1 mark) Total distance traveled.
```{r}
total_distance <- function(x, y) {
  sum(sqrt(diff(x)^2 + diff(y)^2))
}
```

b) (3 marks) Mean absolute change in direction, where direction can be measured as `atan2(dy, dx)` or `atan(dy/dx)`.

```{r}
mean_abs_direction_change <- function(x, y) {
  dx <- diff(x);  dy <- diff(y)
  theta <- atan2(dy, dx)
  mean(abs(diff(theta)))
}
```
c) (4 marks) Maximum absolute curvature, where curvature $\kappa$ is 


$$\kappa = \frac{\det(P', P'')}{||P'||^3} = \frac{dx d^2y - dy d^2x}{((dx)^2 + (dy)^2)^{3/2}}$$
```{r}
max_abs_curvature <- function(x, y) {
  dx  <- diff(x)
  dy  <- diff(y)
  ddx <- diff(dx)
  ddy <- diff(dy)
  num   <- dx[-length(dx)] * ddy - dy[-length(dy)] * ddx
  denom <- (dx[-length(dx)]^2 + dy[-length(dy)]^2)^(3/2)
  max(abs(num / denom))
}
```

d) (6 marks) The 80th percentile of acceleration. The `quantile()` function may be useful in checking your work, but your solution should not include `quantile()`. Instead try, something with `sort()`, `length()`, and `round()`.
```{r}
accel_80th <- function(x, y) {
  v <- sqrt(diff(x)^2 + diff(y)^2)
  a <- abs(diff(v))
  s <- sort(a)
  idx <- ceiling(0.8 * length(s))
  s[idx]
}
```
e) Test you functions on the paths `bezier_path`, `space_path`, and `heart_path`, and fill in the following markdown table to four significant figures. (4 marks)

```{r}
mean_velocity_2d(bezier_path_x, bezier_path_y)
mean_velocity_2d(space_path_x, space_path_y)
mean_velocity_2d(heart_path_x, heart_path_y)

```

|    Function   | Bezier | Space | Heart |
|----------------------------|--------|-------|-------|
| Mean Velocity              | 0.1221 | 1.013 | 1.021 |
| Total Distance             |   12.21 |   244.2    |   102.1    |
| Mean Abs. Direction Change |   0.01821     |   0.3317    |  0.1291     |
| Max. Abs. Curve            |   0.2121     |   39400000    |    4.197   |
| 80th %ile of Acceleration  |      0.001613  | 0.01181      |   0.1226    |


Code:
```{r}

t1 <- seq(0,1, by=0.01)
px <- c(0,3,7,10); py <- c(0,5,5,2)
bezier_x <- (1-t1)^3*px[1] + 3*(1-t1)^2*t1*px[2] + 3*(1-t1)*t1^2*px[3] + t1^3*px[4]
bezier_y <- (1-t1)^3*py[1] + 3*(1-t1)^2*t1*py[2] + 3*(1-t1)*t1^2*py[3] + t1^3*py[4]
t2 <- seq(0,2*pi, length=101)
space_x <- c((t2+4)*sin(3*t2),
             seq(0,30,length=40),
             rev(30 - (t2+1)*sin(3*t2)))
space_y <- c((t2+4)*cos(3*t2),
             seq(10.283,19.283,length=40),
             rev(12 + (t2+1)*cos(3*t2)))
heart_x <- 16*sin(t2)^3
heart_y <- 13*cos(t2) - 5*cos(2*t2) - 2*cos(3*t2) - cos(4*t2)
metrics <- data.frame(
  Bezier = c(
    mean_velocity_2d(bezier_x, bezier_y),
    total_distance(bezier_x, bezier_y),
    mean_abs_direction_change(bezier_x, bezier_y),
    max_abs_curvature(bezier_x, bezier_y),
    accel_80th(bezier_x, bezier_y)
  ),
  Space  = c(
    mean_velocity_2d(space_x, space_y),
    total_distance(space_x, space_y),
    mean_abs_direction_change(space_x, space_y),
    max_abs_curvature(space_x, space_y),
    accel_80th(space_x, space_y)
  ),
  Heart  = c(
    mean_velocity_2d(heart_x, heart_y),
    total_distance(heart_x, heart_y),
    mean_abs_direction_change(heart_x, heart_y),
    max_abs_curvature(heart_x, heart_y),
    accel_80th(heart_x, heart_y)
  ),
  row.names = c(
    "Mean Velocity",
    "Total Distance",
    "Mean Abs. Direction Change",
    "Max. Abs. Curvature",
    "80th %ile Acceleration"
  )
)

knitr::kable(signif(metrics, 4), align="c")


```




Sources:  

Agility & Beyond - Course Map Archives https://www.agilityandbeyond.com/course-map-archive/

Acegikmo - The Beauty of Bezier Curves, https://www.youtube.com/watch?v=aVwxzDHniEw


\newpage

# QUESTION 2 (12 marks): Rural vs. Urban Ontario Part 1 - Power Transforms

Like question 1, this is part 1 of a project that will span 3 or 4 assignments. Here the end goal is become familiar with the data structures that Statistics Canada uses, and learn how we can use these to answer demographic and business questions for both the private and public sectors.



a) (0 marks) Use the following code to find all the census subdivisions (CSDs) in Ontario that are .

```{r}
library(cancensus) # You will need to install.packages("cancensus") one time before this package will load.

all_regions = list_census_regions("CA21") # 2021 census
rural_on_regions = subset(all_regions, 
                          level=="CSD" &  # census subdivisions
                            PR_UID == 35 & # in Ontario
                            is.na(CMA_UID)) # not inside a metropolitan area

csd_list = list(CSD = rural_on_regions$region)

census_data = get_census(dataset = "CA21",
                         regions = csd_list, 
                         level = "CSD")

names(census_data)[4] = "Area"
head(census_data)
```


b) (2 marks) Find the Pearson (default) correlation between the area and the population in rural areas.
```{r}
census_data <- cancensus::get_census(
  dataset = "CA21",
  regions = csd_list,
  level   = "CSD",
)
names(census_data)[names(census_data) == "Area (sq km)"] <- "Area"
names(census_data)[names(census_data) == "Population"]  <- "Pop2021"

cor(census_data$Area, census_data$Pop2021, use = "complete.obs")
```
c) (4 marks) Plot the scatterplot between population (y) over area (x) of rural CSDs and use what you see to make an argument as to what kinds of power transformations on x and y are likely to increase the correlation coefficient.
```{r plot-area-pop, message=FALSE, warning=FALSE, fig.height=4.5, fig.width=6}
library(ggplot2)

ggplot(census_data, aes(Area, Pop2021)) +
  geom_point(alpha = 0.6) +
  labs(x = "Area (sq km)", y = "Population (2021)",
       title = "Rural Ontario CSDs: Population vs Area") +
  theme_minimal()
```

This above plot is okay but not obvious for a visible pattern, so the below one is a plot that only shows CSDs whose area falls between 0 and 2 000. Larger areas exist but are hidden. This is only for the usage of showing the pattern.
```{R}
ggplot(census_data, aes(Area, Pop2021)) +
  geom_point(size = 1, colour = "steelblue", alpha = 0.7) +
  coord_cartesian(xlim = c(0, 2000)) +
  labs(x = "Area (sq km)", y = "Population 2021") +
  theme_minimal()

```

Inside that narrow x-range population varies from near zero to tens of thousands which creates a wedge or fan shape, wider at the top than at the bottom. The sparse points on the far right are the very large but lightly populated CSDs.

The scatterplot looks like a fan that opens wider as area increases. Small-area CSDs range from almost nobody to modest towns, while the largest areas are consistently sparse. This suggests a multiplicative, power-law link instead of a straight line. Applying natural or base-10 logarithms to both variables compresses the extreme values, stretches the small ones, and turns the curved cloud into a nearly straight band.
After part d, I find that because Pearson r measures linear association, the log–log transformation should raise the coefficient most. A gentler option is to use a square-root on area together with a log on population, but logs on both variables are likely to give the largest improvement.

If we put the scale into base-10 logarithmic scales, we are able to see all the values in a reasonable way:
```{R}
library(scales)

ggplot(subset(census_data, Area > 0 & Pop2021 > 0),
       aes(Area, Pop2021)) +
  geom_point(size = 1, colour = "steelblue", alpha = 0.7) +
  scale_x_log10(labels = comma) +
  scale_y_log10(labels = comma) +
  labs(x = "Area (sq km, log scale)",
       y = "Population 2021 (log scale)") +
  theme_minimal()
```


d) (6 marks) Try all combinations of the following power transformations on area and population on rural areas, and find the best correlation coefficient amongst transformed datasets.

Show your code and report what combination gives the strongest correlation and what that correlation is.

The power transformations are `log(x)`, `sqrt(x)`, `-1/x`, `x^2`, and `x^3`.

Design:
Filter out rows that would break a transform (for example log and sqrt need positive values).

Write a named list of the five transform functions.

Loop over every area-population pair, compute cor(), and store the result.

Identify the combination with the highest |r|

```{R}

library(dplyr)
dat <- census_data %>% filter(Area > 0, Pop2021 > 0)
tf <- list(
  log  = log,
  sqrt = sqrt,
  inv  = function(z) -1 / z,
  sq   = function(z) z^2,
  cube = function(z) z^3
)
cmb <- expand.grid(area = names(tf), pop = names(tf), stringsAsFactors = FALSE)
cmb$r <- mapply(function(a, p) {
  cor(tf[[a]](dat$Area), tf[[p]](dat$Pop2021), use = "complete.obs")
}, cmb$area, cmb$pop)
cmb[which.max(abs(cmb$r)), ]

```


That means, log-log gives the strongest correlation and the value is 0.5067254




\newpage

# QUESTION 3 (10 marks): Reading assignment - Quantum computing applied to Stock Portfolios



Read pages 1-15 of the article "Portfolio Optimization of 60 Stocks Using Classical and Quantum Algorithms" by the group Chicago Quantum and use it to answer the following questions. The article is found in Learn in the same folder as this assignment.


Unless a question says "in your own words", you may take quotes directly from the article without citation.

For your interest, QUBO stands for Quadratic unconstrained binary optimization, even though it is never explicitly stated in the article. You shouldn't need this information to answer the questions, but the term comes up enough that you should know.



Questions:

a) (2 marks) In your own words. Assuming that the Chicago Quantum Ratio (CQR) is meant to be maximized, what two features of the ideal stock portfolio does this imply?

The Chicago Quantum Ratio (CQR) is defined as:

$$
\mathrm{CQR}(R_w)
\;=\;
\frac{\mathrm{Cov}_{im}(R_w)}{\sigma(R_w)}
$$

which means the optimal portfolio must strike two goals simultaneously:


1) It should have a high market covariance and load strongly in the same direction as overall market returns (S&P 500), so \[\mathrm{Cov}(R_w, R_m)\] is large.

2) It has a low portfolio risk, so it must keep its own volatility \[\sigma(R_w)\] as small as possible.

b) (1 mark) What is the size of the population of candiate 60-asset portfolios?


```{r}
choices <- 2^60
choices
```


c) (2 marks) In your own words (30 words or fewer), how does the genetic algorithm being applied to stocks work in this instance. Not every detail is needed.

The genetic algorithm maintains a pool of high-scoring portfolios, breeds them into offspring with crossover and mutation, then iterates for 40 generations, which always keeping the top 40 survivors each round.

d) (1 mark) Does a hotter 'temperature' in a simulated annealer imply that the solution is more or less likely to settle into a local minima? No explanation is necessary.

A hotter temperature makes it less likely to settle into a local minimum, since it more readily jumps out of shallow wells.

e) (1 mark) Is Chicago Quantum Net Score being minimized or maximized by the D-Wave system in this application? No explanation is necessary.

The D-Wave system minimizes the Chicago Quantum Net Score (CQNS) by finding low-energy QUBO solutions.

f) (1 mark) What tendency did the D-Wave system have when finding portfolios that differed from what was desired? No explanation is necessary.

When target size grows large (above ~36 stocks), D-Wave returns portfolios smaller than requested, often capping around 47 assets.

g) (2 marks) In your own word, in comparative analysis, how well did the D-Wave system do at finding the best possible portfolio?

Across the five classical heuristics and D-Wave, six methods (including D-Wave) found the ideal CQNS-minimizing portfolio. D-Wave did so after sampling only 2588 valid portfolios, placing its points squarely on the efficient frontier—matching or exceeding the coverage achieved by Monte Carlo (221660 samples) and the genetic algorithm (40000 samples) .
