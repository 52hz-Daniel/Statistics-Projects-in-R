---
title: "STAT 341 - Assignment 4 - Spring 2025"
date: 'Due: Friday, Aug 1, 2025 at 11:59'
output:
  pdf_document: default
  word_document: default
  html_document: default
---



Your assignment must be submitted by the due date listed at the top of this document, and it must be submitted electronically in .pdf format via Crowdmark. This means that your responses for different questions should begin on separate pages of your .pdf file. Note that your .pdf solution file must have been generated by R Markdown. Additionally:

* For mathematical questions: your solutions must be produced by LaTeX (from within R Markdown). Neither screenshots nor scanned/photographed handwritten solutions will be accepted -- these will receive zero points.

* For computational questions: R code should always be included in your solution (via code chunks in R Markdown). If code is required and you provide none, you will receive zero points.

* For interpretation questions: plain text (within R Markdown) is required. Text responses embedded as comments within code chunks will not be accepted.

Organization and comprehensibility is part of a full solution. Consequently, points will be deducted for solutions that are not organized and incomprehensible. Furthermore, if you submit your assignment to Crowdmark, but you do so incorrectly in any way (e.g., you upload your Question 2 solution in the Question 1 box), you will receive a 5% deduction (i.e., 5% of the assignment’s point total will be deducted from your point total).



\newpage

## QUESTION 1 [20 marks] - Multiple Testing and Transformed Data

In class, we did random mixing of two groups to get a distribution of the attributes of the sampling distribution if the null hypothesis "these two subpopulations are indistinguishable". This was part of a set of randomization tests to estimate p-values for different attributes about the subpopulations, and we used a "minimum of p-values" method to account for the multiple testing problem when those attributes might be correlated.

In this assignment we will apply a similar random mixing approach, and the same 'minimum of p-values' approach to testing for a correlation between two variables under different transformations. To do this, we want some version of random mixing that would produce a distribution of attributes under the null hypothesis "variables x and y are independent". We will do that by starting from an observed set of ordered pairs $(x_i, y_i), i=1, \dots, 24$ and producing random pairings by randomly mixing the y values with the x values.

The following function will be valuable to you in this question.

```{r}
randomPairing = function(x,y)
{
  xrand = sample(x)
  yrand = sample(y)
  
  return(list(x=xrand, y=yrand))
}
```

The following dataset will also be useful.

```{r}
df = read.csv("Stat341_S25_LinkedIn_Dataset.csv")
head(df)
dim(df)
```

a) (2 marks) Use the `cor` function to find the Pearson correlation between `Impressions` (y), and `Reacts` (x). Put this estimate into the t-test for testing if a Pearson correlation is zero. Use a two-tailed test, and compare against the $T_{n-2}$ distribution.

$$H_o: \rho = 0,\ \ H_A:\rho \neq 0\ \ \ \ \ \ t_{n-2, obs} = r\sqrt{\frac{n-2}{1-r^2}}$$

```{r}



x <- df$Reacts
y <- df$Impressions
n <- length(x)

r_xy <- cor(x, y, method = "pearson")
t_obs <- r_xy * sqrt((n-2)/(1-r_xy^2))
p_val <- 2 * pt(-abs(t_obs), df = n-2)

r_xy
t_obs
p_val

```
Since p-value < 0.05, we reject the null hypothesis that the true correlation is zero. We have strong evidence that the true correlation is not zero.


b) (3 marks) Apply the transformations in the following table to both variables of the data and compute the sample correlation $r$ as well as the t-score and p-value as you did in part a.


| Transformation of x,y | Correlation r | Test Statistic t | p-value |
|-----------------------|---------------|------------------|---------|
| x, y                  | 0.5818502	              |      3.355629            |      2.858144e-03	   |
| log(x+1), log(y+1)    |       0.7467476	        |         5.266151	         |   2.772291e-05	      |
| sqrt(x), sqrt(y)      |         0.6663445	      |             4.191588	     |     3.778853e-04	    |
| -1/(x+1), -1/(y+1)    |    0.7581940	           |            5.454121      |       1.767412e-05	  |

```{r}
transformations <- list(
  identity = function(x) x,
  log1p = function(x) log(x + 1),
  sqrt = function(x) sqrt(x),
  neg_inv = function(x) -1/(x+1)
)

results <- data.frame(
  Transformation = character(),
  r = numeric(),
  t = numeric(),
  p_value = numeric(),
  stringsAsFactors = FALSE
)

for (name in names(transformations)) {
  xt <- transformations[[name]](x)
  yt <- transformations[[name]](y)
  r <- cor(xt, yt)
  t <- r * sqrt((n-2)/(1-r^2))
  p <- 2*pt(-abs(t), df=n-2)
  
  results <- rbind(results, data.frame(Transformation=name, r=r, t=t, p_value=p))
}

results

```



c) (4 marks) Use the `randomPairing` function to produce a random pairing of the untransformed x and y. Plot in a 2x2 grid using `par(mfrow = c(2,2))` the following plots:  

- Top left - Histogram of observed x, `df$Reacts`
- Top right - Histogram of observed y, `df$Impressions`
- Bottom left - Histogram of randomPairing x, `rp$x``
- Bottom right - Histogram of observed x, `rp$y``

Very briefly comment on the suitability of a transformation in each situation.

```{r}

set.seed(1)
rp <- randomPairing(x, y)

par(mfrow=c(2,2))
hist(x, main="Observed Reacts", xlab="Reacts")
hist(y, main="Observed Impressions", xlab="Impressions")
hist(rp$x, main="Random Pairing x", xlab="x")
hist(rp$y, main="Random Pairing y", xlab="y")

```
Reacts (x): The distribution is fairly spread out but not very skewed, so transformations like sqrt() or log(x+1) could smooth it slightly but may not be strictly necessary.

Impressions (y): This is strongly right-skewed, with a long tail. A transformation like log(y + 1) or sqrt(y) is highly appropriate to normalize or stabilize the variance.

Therefore,

Use a log or square root transformation for Impressions.

Consider leaving Reacts as is, or applying sqrt() if needed for modeling consistency.




d) (4 marks) Generate 5000 random Pairings of the untransformed x and y, and find the t-score of each one. Plot a histogram of the 5000 t-scores with a red (`col="Red`) vertical line of width (`lwd = 3`) with `abline(v=..., col="Red")`.

```{r}
set.seed(2)
B <- 5000
t_scores <- numeric(B)

for (b in 1:B) {
  rp <- randomPairing(x, y)
  r_b <- cor(rp$x, rp$y)
  t_scores[b] <- r_b * sqrt((n-2)/(1-r_b^2))
}

hist(t_scores, breaks=50, main="Histogram of 5000 Random t-scores", xlab="t-score", xlim = c(-3.5, 4))
abline(v = t_obs, col="Red", lwd=3)



```


e) (1 mark) Empirically estimate the p-value from the observed t-score, and the 5000 random pairings. (Remember that you're doing a two-tailed test)

Looking at your plot, t_obs (red line) is far out in the right tail, around 3.36. The histogram ends around 3, so very few (possibly 0 or 1) of the 5000 simulated t-scores are that large in magnitude.

So the empirical p-value is likely very small, possibly around $$\frac{1}{5000} = 0.0002$$ or even 0, suggesting strong evidence against the null hypothesis.

By R code, the calculation is:
```{r}

emp_p <- mean(abs(t_scores) >= abs(t_obs))
emp_p

```

f) (4 marks) For the four transforms in part b (including the identity transformation), on 5000 random pairings, find the theoretical p-values againist the null of no correlation for each transformation. Draw a scatterplot of the p-values of the untransformed random pairings in the x, and the log-transformed random pairings in the y *for the first 500 values of each*. (Note: This means that for each random pairing you should find the correlation four times and get four p-values. There should be 5000 pairings in total, not 20,000.)


```{r}

B <- 5000
pvals_matrix <- matrix(NA, nrow=B, ncol=4)

for (b in 1:B) {
  rp <- randomPairing(x, y)
  col_idx <- 1
  for (name in names(transformations)) {
    xt <- transformations[[name]](rp$x)
    yt <- transformations[[name]](rp$y)
    r_b <- cor(xt, yt)
    t_b <- r_b * sqrt((n-2)/(1-r_b^2))
    pvals_matrix[b, col_idx] <- 2*pt(-abs(t_b), df=n-2)
    col_idx <- col_idx + 1
  }
}

colnames(pvals_matrix) <- names(transformations)
plot(pvals_matrix[1:500, "identity"], pvals_matrix[1:500, "log1p"],
     xlab="Untransformed p-value", ylab="Log-transformed p-value", main="Scatterplot of p-values")

```

g) (2 marks) For each of the 5000 sets of p-values in part f, find the minimum of the four p-values from that set. Report the proportion of times that the minimum p-value from the random pairings was lower than the minimum p-value from the observed data (you should be able to get the observed minimum p-value from your answer in part b)

This proportion is your estimate of the p-value adjusted for testing across multiple transformations.

You may find either the `pmin()` function or the `apply()` function valuable. 


```{r}
obs_min_p <- min(results$p_value)
min_pvals <- apply(pvals_matrix, 1, min)

prop_lower <- mean(min_pvals <= obs_min_p)
prop_lower

```

Among the 5000 random pairings, none produced a minimum p value as small as the observed minimum p value from the real data. That is extremely strong evidence against the null of no correlation under any of the four transforms.


\newpage

## QUESTION 2 [15 marks]: A copy, of a copy, of a copy, of a legal document.

In this question you will explore how well (or poorly) a set of bootstrap resamples and double bootstrap re-resamples preserves the sampling distribution of the original study population.

The $y$ values of interest of the number of citations found in-network (that is, a small portion of the Canadian Legal Landscape) from a 2015 snapshot of the Supreme Court of Canada decisions found from CanLII (Canadian Legal Information Institute).

The dataset where you can find this is `Lawnodes.csv` and the variables of interest are the (in-network) citation count `Citecount`, and the identifying code, `ID`.

```{r}
df_law = read.csv("Lawnodes.csv")
#df_law$idx = 1:nrow(df_law)
hist(df_law$citecount)
```

We are going to use the following samples of size 5, 10, 20, and 50.

```{r}
set.seed(341)
df_S05 = df_law[sample(1:nrow(df_law), 5) , c("ID","citecount")]
df_S10 = df_law[sample(1:nrow(df_law), 10) , c("ID","citecount")]
df_S20 = df_law[sample(1:nrow(df_law), 20) , c("ID","citecount")]
df_S50 = df_law[sample(1:nrow(df_law), 50) , c("ID","citecount")]
```


a) (2 marks)  Find the sample mean, variance, and number of unique values (unique-Ncite) of `citecount` in the population and each of the samples, and fill the following table. You may use `mean()`, `var()`, and `length(unique())` to find these.



```{r}

summary_stats <- function(v) {
  c(mean = mean(v),
    variance = var(v),
    unique_Ncite = length(unique(v)))
}

pop_stats  <- summary_stats(df_law$citecount)
s5_stats   <- summary_stats(df_S05$citecount)
s10_stats  <- summary_stats(df_S10$citecount)
s20_stats  <- summary_stats(df_S20$citecount)
s50_stats  <- summary_stats(df_S50$citecount)

knitr::kable(rbind(
  Population     = pop_stats,
  `Sample size 5`  = s5_stats,
  `Sample size 10` = s10_stats,
  `Sample size 20` = s20_stats,
  `Sample size 50` = s50_stats
), digits = 2)

```


b) (2 marks) From the sample of size 5, resample $B=1000$ times with replacement using the following code. Report in a 2x2 histogram, the following: resampled means, variances, number of unique citation counts, number of unique IDs.


```{r}
set.seed(341)
B=1000
S_idx = 1:5
n = length(S_idx)
means = rep(NA,B)
vars = rep(NA,B)
uniques_Ncite = rep(NA,B)
uniques_ID = rep(NA,B)

for(k in 1:B)
{
  Sstar_idx = sample(S_idx, n, replace = TRUE)
  sample_rows <- df_S05[Sstar_idx, ] 
  means[k]         = mean(sample_rows$citecount)
  vars[k]          = var(sample_rows$citecount)
  uniques_Ncite[k] =  length(unique(sample_rows$citecount))
  uniques_ID[k]    =  length(unique(sample_rows$ID))
}

par(mfrow = c(2,2))
hist(means, main="Resampled Means")
hist(vars, main="Resampled Variances")
hist(uniques_Ncite, main="Resampled Unique Citation Counts")
hist(uniques_ID, main="Resampled Unique Legal Decision")
par(mfrow = c(1,1))

```





c) (2 marks) Repeat part b for the sample of size 50.

```{r}
set.seed(341)
B=1000
S_idx = 1:50
n = length(S_idx)
means = rep(NA,B)
vars = rep(NA,B)
uniques_Ncite = rep(NA,B)
uniques_ID = rep(NA,B)

for(k in 1:B)
{
  Sstar_idx = sample(S_idx, n, replace = TRUE)
  sample_rows <- df_S50[Sstar_idx, ] 
  means[k]         = mean(sample_rows$citecount)
  vars[k]          = var(sample_rows$citecount)
  uniques_Ncite[k] =  length(unique(sample_rows$citecount))
  uniques_ID[k]    =  length(unique(sample_rows$ID))
}

par(mfrow = c(2,2))
hist(means, main="Resampled Means")
hist(vars, main="Resampled Variances")
hist(uniques_Ncite, main="Resampled Unique Citation Counts")
hist(uniques_ID, main="Resampled Unique Legal Decision")
par(mfrow = c(1,1))

```


d) (3 marks) From the sample of size 5, resample $B=1000$ times with replacement, and re-resample each of these $D=100$ times using the following code. Report in a 2x2 histogram, the following: re-resampled means, variances, number of unique citation counts, number of unique IDs.



```{r}
set.seed(341)
B=1000
D=100
S_idx = 1:5
n = length(S_idx)
means = rep(NA,B*D)
vars = rep(NA,B*D)
uniques_Ncite = rep(NA,B*D)
uniques_ID = rep(NA,B*D)

for(k in 1:B)
{
  Sstar_idx = sample(S_idx, n, replace = TRUE)
  
  for(l in 1:D)
  {
    idx <- l + (k - 1) * D
    Sstarstar_idx <- sample(Sstar_idx, n, replace = TRUE)
    sample_rows <- df_S05[Sstarstar_idx, ]
    means[idx]         = mean(sample_rows$citecount)
    vars[idx]          = var(sample_rows$citecount)
    uniques_Ncite[idx] = length(unique(sample_rows$citecount))
    uniques_ID[idx]    = length(unique(sample_rows$ID))
  }
}

par(mfrow = c(2,2))
hist(means, main="Resampled Means")
hist(vars, main="Resampled Variances")
hist(uniques_Ncite, main="Resampled Unique Citation Counts")
hist(uniques_ID, main="Resampled Unique Legal Decision")
par(mfrow = c(1,1))

```

e) (2 marks) Repeat part b for the sample of size 50.

```{r}
set.seed(341)
B=1000
D=100
S_idx = 1:50
n = length(S_idx)
means = rep(NA,B*D)
vars = rep(NA,B*D)
uniques_Ncite = rep(NA,B*D)
uniques_ID = rep(NA,B*D)

for(k in 1:B)
{
  Sstar_idx = sample(S_idx, n, replace = TRUE)
  
  for(l in 1:D)
  {
    idx <- l + (k - 1) * D
    Sstarstar_idx <- sample(Sstar_idx, n, replace = TRUE)
    sample_rows <- df_S50[Sstarstar_idx, ]
    means[idx]         = mean(sample_rows$citecount)
    vars[idx]          = var(sample_rows$citecount)
    uniques_Ncite[idx] = length(unique(sample_rows$citecount))
    uniques_ID[idx]    = length(unique(sample_rows$ID))
  }
}

par(mfrow = c(2,2))
hist(means, main="Resampled Means")
hist(vars, main="Resampled Variances")
hist(uniques_Ncite, main="Resampled Unique Citation Counts")
hist(uniques_ID, main="Resampled Unique Legal Decision")
par(mfrow = c(1,1))

```

f) (2 marks) Comment on how well (or poorly) the sample of size 5 maintains its ability to describe the data when being resampled and re-resampled?

The original sample has at most 5 unique IDs and a small number of unique citation counts. In the bootstrap histograms, the distributions of means and variances are very wide and jagged because resamples can heavily repeat individual cases. Unique citation counts and IDs are often low (1–4), so the resamples cannot reflect the full variability or long tail of the population.

In the double bootstrap, the discreteness becomes even stronger. Means and variances show irregular, spiky distributions. Unique counts remain low because the process keeps reusing the same few observations.

Conclusion: The size 5 sample fails to maintain a representative picture of the population when resampled and especially when re-resampled. Results are highly variable and biased toward the small number of values seen initially.


g) (2 marks) Comment on how well (or poorly) the sample of size 50 maintains its ability to describe the data when being resampled and re-resampled? Include a comparison to the size 5 sample, if appropriate.


In the bootstrap histograms, the means and variances are much smoother, and the center of the distribution is closer to the population values. Unique citation counts and IDs are much higher (close to 50). The resamples reflect more of the population’s diversity.

In the double bootstrap, the distributions remain smooth and centered, though slightly narrower due to reusing observations in the inner loop.

Compared to size 5, the size 50 sample better captures the heavy-tailed nature of the citation counts. It produces bootstrap distributions that resemble what we’d expect from the true population, and retains a high number of unique values, avoiding the extreme discreteness problem seen in size 5.

Conclusion: Size 50 maintains its descriptive power far better than size 5 during resampling and re-resampling. The larger sample leads to more stable and reliable bootstrap inference.


\newpage



## QUESTION 3: Bootstrap Confidence Intervals [25 marks]

In this question, you will turn your focus to (point and interval) estimation of the *treatment effect*, a quantification of the impact on `dissent` on `citecount`. That is, the effect that the existence of a minority dissenting opinion in the Supreme Court of Canada over a decision has on the number of times the decision is cited later (within the network measured). Treatment impact can be quantified in a variety of ways; here you will focus on the *average treatment effect* 

$$\text{ATE} = \overline{y}_T-\overline{y}_C$$ 

and also the percent treatment effect, referred to in A/B testing settings as *lift*: 

$$\text{lift} = \frac{\overline{y}_T-\overline{y}_C}{\overline{y}_C}$$

where 

$$\overline{y}_T = \frac{1}{n_T}\sum_{u\in\mathcal{S}_T}y_u ~~~~ \text{and} ~~~~ \overline{y}_C = \frac{1}{n_C}\sum_{u\in\mathcal{S}_C}y_u$$

are the average citation counts in the treatment (dissenting) and control (unanimous) groups, respectively.



(a) (4 marks) Consider the following simple linear regression model 

$$y_u = \alpha + \beta~x_u+r_u, ~~~~ u\in\mathcal{S}=\mathcal{S}_T\cup\mathcal{S}_C$$

where $y_u$ represents the `citecount` of decision $u$ and $x_u$ is a treatment assignment indicator defined as follows: $$x_u =
  \begin{cases}
    1 & \text{if decision}~u~\text{had dissent} \\
    0 & \text{if decision}~u~\text{was unanimous}
  \end{cases}$$ 
  
  Earlier in the course, it was established that the least squares estimates of $\alpha$ and $\beta$ are 
  
$$\hat\alpha=\overline{y}-\hat\beta~\overline{x} ~~~~ \text{and} ~~~~ \hat\beta=\frac{\sum_{u\in\mathcal{S}}(x_u-\overline{x})(y_u-\overline{y})}{\sum_{u\in\mathcal{S}}(x_u-\overline{x})^2}$$ where $\overline{x} = \sum_{u\in\mathcal{S}}x_u/n$, $\overline{y} = \sum_{u\in\mathcal{S}}y_u/n$, and $n= n_T + n_C$. 
  
$\;$  
  
For $x_u$ and $y_u$ as defined above, show that $$\hat\alpha = \overline{y}_C~~~~ \text{and} ~~~~\hat\beta=\overline{y}_T-\overline{y}_C$$ and hence that $\text{ATE}=\hat\beta$ and $\text{lift}=\hat\beta/\hat\alpha$.
 
    **Hints:**

    * $\sum_{u\in\mathcal{S}}x_u = n_T$
    * $\sum_{u\in\mathcal{S}}y_ux_u = \sum_{u\in\mathcal{S}_T}y_u = n_T\overline{y}_T$
    * $x_u^2=x_u ~~\forall~ u$

$\;$


\newpage


(b) (2 marks) Using the `lm()` function, fit the linear regression model above and calculate the $\text{ATE}$ and $\text{lift}$ for this (post-hoc) A/B test.

```{r}
S <- df_law[, c("citecount", "dissent")]

fit <- lm(citecount ~ dissent, data = S)

alpha_hat <- coef(fit)[["(Intercept)"]]
beta_hat  <- coef(fit)[["dissent"]]
ATE  <- beta_hat
lift <- beta_hat / alpha_hat
c(ATE = ATE, lift = lift)



```


$\;$

(c) (2 marks) By resampling $\mathcal{S}$ with replacement, construct $B=1000$ bootstrap samples $\mathcal{S}_1^\star,\mathcal{S}_2^\star,\ldots,\mathcal{S}_{1000}^\star$. Print out your code, not the 1000 samples. Note that the units in $\mathcal{S}$ should be regarded as the $(x_u,y_u)$ pairs. Thus the pairs themselves should be resampled with replacement.

```{r}
set.seed(123)
B <- 1000
n <- nrow(S)

boot_samples <- replicate(
  B,
  S[sample.int(n, n, replace = TRUE), c("citecount", "dissent")],
  simplify = FALSE
)

```



$\;$

(d) (6 marks) For each of the $B=1000$ bootstrap samples in part (c), use `lm()` to calculate the least squares estimates $\hat\alpha^\star$ and $\hat\beta^\star$ and hence $\text{ATE}^\star$ and hence $\text{lift}^\star$ in each bootstrap sample $\mathcal{S}_b^\star, ~~ b=1,2,...,B.$ Construct two histograms, one of the $\text{ATE}^\star$ values and the other of the $\text{lift}^\star$ values. Include a vertical line representing $\text{ATE}$ from (b) on the first histogram and a line representing $\text{lift}$ from (b) in the second. Be sure to informatively label your plots.

```{r}
ate_from_df <- function(df) {
  if (!any(df$dissent == 1) || !any(df$dissent == 0)) return(NA_real_)
  mean(df$citecount[df$dissent == 1]) - mean(df$citecount[df$dissent == 0])
}

lift_from_df <- function(df) {
  if (!any(df$dissent == 1) || !any(df$dissent == 0)) return(NA_real_)
  yT <- mean(df$citecount[df$dissent == 1])
  yC <- mean(df$citecount[df$dissent == 0])
  (yT - yC) / yC
}

ATE_star  <- vapply(boot_samples, ate_from_df,  numeric(1))
lift_star <- vapply(boot_samples, lift_from_df, numeric(1))

ATE_star  <- ATE_star[!is.na(ATE_star)]
lift_star <- lift_star[!is.na(lift_star)]

hist(ATE_star, breaks = 40, main = "Bootstrap distribution of ATE*",
     xlab = "ATE*")
abline(v = ATE, lwd = 2)

hist(lift_star, breaks = 40, main = "Bootstrap distribution of lift*",
     xlab = "lift*")
abline(v = lift, lwd = 2)

```


$\;$

(e) (2 marks) Calculate 95% confidence intervals for $\text{ATE}$ and $\text{lift}$ using the naive normal theory approach. 


```{r}
se_beta <- sqrt(vcov(fit)["dissent", "dissent"])
z <- qnorm(0.975)
CI_ATE_naive <- c(ATE - z * se_beta, ATE + z * se_beta)

V <- vcov(fit)[c("(Intercept)", "dissent"), c("(Intercept)", "dissent")]
alpha <- alpha_hat; beta <- beta_hat

grad <- c(-beta / alpha^2, 1 / alpha)

se_lift <- sqrt(t(grad) %*% V %*% grad)
CI_lift_naive <- c(lift - z * se_lift, lift + z * se_lift)

CI_ATE_naive
CI_lift_naive

```


$\;$

(f) (2 marks) Calculate 95% confidence intervals for $\text{ATE}$ and $\text{lift}$ using the percentile method.
```{r}

CI_ATE_pct  <- quantile(ATE_star,  probs = c(0.025, 0.975), na.rm = TRUE)
CI_lift_pct <- quantile(lift_star, probs = c(0.025, 0.975), na.rm = TRUE)

CI_ATE_pct
CI_lift_pct

```
$\;$

(g) (2 marks) Calculate 95% confidence intervals for $\text{ATE}$ and $\text{lift}$ using the bootstrap-$t$ approach. You should use the `bootstrap_t_interval_new()` function included in the Appendix. Note that this is a modified version of the `bootstrap_t_interval()` function from class that accommodates a data frame (instead of a vector) for the input `S`. For the input `a`, you will also need to write functions that calculate $\text{ATE}$ and $\text{lift}$ given a data frame that includes columns for `citecount` and `dissent`. Please use $B=100$ and $D=100$.

```{r}

ate_from_df <- function(df) {
  if (!any(df$dissent == 1) || !any(df$dissent == 0)) return(NA_real_)
  mean(df$citecount[df$dissent == 1]) - mean(df$citecount[df$dissent == 0])
}
lift_from_df <- function(df) {
  if (!any(df$dissent == 1) || !any(df$dissent == 0)) return(NA_real_)
  yT <- mean(df$citecount[df$dissent == 1]); yC <- mean(df$citecount[df$dissent == 0])
  (yT - yC) / yC
}

# With some modification on the provided function
bootstrap_t_interval <- function(S, a, confidence, B, D) {
  Pstar <- S
  aPstar <- a(Pstar)
  sampleSize <- nrow(S)
  
  bVals <- sapply(1:B, FUN = function(b) {

    Sstar <- Pstar[sample.int(sampleSize, sampleSize, replace = TRUE), , drop = FALSE]
    aSstar <- a(Sstar)

    Pstarstar <- Sstar
    inner <- sapply(1:D, FUN = function(d) {
      Sstarstar <- Pstarstar[sample.int(sampleSize, sampleSize, replace = TRUE), , drop = FALSE]
      a(Sstarstar)
    })
    SD_aSstar <- sd(inner, na.rm = TRUE)
    z <- (aSstar - aPstar) / SD_aSstar
    c(aSstar = aSstar, z = z)
  })
  SDhat <- sd(bVals[1, ], na.rm = TRUE)
  zVals <- bVals[2, ]
  
  cValues <- quantile(zVals, probs = c((1 - confidence)/2, (1 + confidence)/2), na.rm = TRUE)
  cLower <- min(cValues); cUpper <- max(cValues)
  
  c(lower = aPstar - cUpper * SDhat,
    middle = aPstar,
    upper = aPstar - cLower * SDhat)
}

set.seed(123)
CI_ATE_bt  <- bootstrap_t_interval(S, ate_from_df,  confidence = 0.95, B = 100, D = 100)
CI_lift_bt <- bootstrap_t_interval(S, lift_from_df, confidence = 0.95, B = 100, D = 100)

CI_ATE_bt
CI_lift_bt


```




$\;$

(h) (3 marks) This question concerns advantages and disadvantages associated with the various methods of confidence interval calculation you've explored.

    i. (1 mark) List one advantage and one disadvantage of naive normal theory intervals.
    
    Advantage: Simple, very fast, uses familiar regression output.
    
    Disadvantage: Relies on approximate normality and a good standard error estimate. For lift, normal theory requires a delta method that can be inaccurate with skew or small samples.

    ii. (1 mark) List one advantage and one disadvantage of percentile method intervals.
    
    Advantage: Distribution free. Captures skew and nonlinearity of the statistic automatically.
    
    Disadvantage: Can be biased when the statistic is biased or when variability depends strongly on the parameter. Coverage can be off in small samples.

    iii. (1 mark) List one advantage and one disadvantage of bootstrap-$t$ intervals.
    
    Advantage: Adjusts for scale and skew by studentizing. Often gives better coverage than percentile for difficult statistics such as ratios.
    
    Disadvantage: Computationally heavier, and can be unstable if inner SD estimates are noisy or if some bootstrap samples miss a group.


$\;$

$\;$


\newpage


## OPTIONAL QUESTION 1: Practice Interview Questions [0 points]

Here are four very common data science interview questions related to the material covered on this assignment. In preparation for your own interviews, or the final exam perhaps, think about how you would answer them. In all cases, imagine you're discussing these things with the Senior Data Scientist who is interviewing you. You do not need to submit answers for these.

(a) [0 points] In your own words, explain what a p-value is.

(b) [0 points] In your own words, explain what the randomization test is, and why it's useful.

(c) [0 points] In your own words, explain what it means to be "95% confident" in a 95% confidence interval. 

(d) [0 points] In your own words, explain what resampling methods like the bootstrap are, and why they are useful. 



\newpage

## USEFUL FUNCTIONS

Note that in the functions below the input `pop` must be a list containing two elements, `pop1` and `pop2` which are themselves data frames containing the data corresponding to sub-populations $\mathcal{P}_1$ and $\mathcal{P}_2$, respectively. Also note that `calculatePVmulti` below works exactly the same as `calculateSLmulti` from Section 4.2.3 of the lecture notes.

```{r}
calculatePVmulti <- function(pop, discrepancies, M_outer, M_inner) {

  ## Local function to calculate the significance levels over the discrepancies
  ## and return their minimum
  
  getPVmin <- function(basePop, discrepancies, M) {
    observedVals <- sapply(discrepancies, FUN = function(discrepancy) {
      discrepancy(basePop)
    })
    
    K <- length(discrepancies)
    
    total <- Reduce(function(counts, i) {
      # mixRandomly mixes the two populations randomly, so the new sub-populations
      # are indistinguishable
      NewPop <- mixRandomly(basePop)
      
      ## calculate the discrepancy and counts
      Map(function(k) {
        Dk <- discrepancies[[k]](NewPop)
        if (Dk >= observedVals[k]) 
          counts[k] <<- counts[k] + 1
      }, 1:K)
      counts
    }, 1:M, init = numeric(length = K))
    
    PVs <- total/M
    min(PVs)
  }
  
  PVmin <- getPVmin(pop, discrepancies, M_inner)
  
  total <- Reduce(function(count, m) {
    basePop <- mixRandomly(pop)
    if (getPVmin(basePop, discrepancies, M_inner) <= PVmin) 
      count + 1 else count
  }, 1:M_outer, init = 0)
  
  PVstar <- total/M_outer
  return(PVstar)
}
```

\newpage

$\;$

```{r}
mixRandomly <- function(pop) {
    pop1 <- pop$pop1
    n_pop1 <- nrow(pop1)
    
    pop2 <- pop$pop2
    n_pop2 <- nrow(pop2)
    
    mix <- rbind(pop1, pop2)
    select4pop1 <- sample(1:(n_pop1 + n_pop2), n_pop1, replace = FALSE)
    
    new_pop1 <- mix[select4pop1, ]
    new_pop2 <- mix[-select4pop1, ]
    list(pop1 = new_pop1, pop2 = new_pop2)
}
```

\newpage 

```{r}
bootstrap_t_interval <- function(S, a, confidence, B, D) {
    ## Inputs: 
    ##    S = an n element array containing the variate values in the sample 
    ##    a = a scalar-valued function that calculates the attribute a() of interest 
    ##    confidence = a value in (0,1) indicating the confidence level 
    ##    B = a numeric value representing the outer bootstrap count of
    ##    replicates (used to calculate the lower and upper limits) 
    ##    D = a numeric value representing the inner bootstrap count of replicates
    ##    (used to estimate the standard deviation of the sample attribute for
    ##    each (outer) bootstrap sample)
    
    Pstar <- S
    aPstar <- a(Pstar)
    sampleSize <- length(S)
    ## get (outer) bootstrap values
    bVals <- sapply(1:B, FUN = function(b) {
        Sstar <- sample(Pstar, sampleSize, replace = TRUE)
        aSstar <- a(Sstar)
        ## get (inner) bootstrap values to estimate the SD
        Pstarstar <- Sstar
        SD_aSstar <- sd(sapply(1:D, FUN = function(d) {
            Sstarstar <- sample(Pstarstar, sampleSize, replace = TRUE)
            ## return the attribute value
            a(Sstarstar)
        }))
        z <- (aSstar - aPstar)/SD_aSstar
        ## Return the two values
        c(aSstar = aSstar, z = z)
    })
    SDhat <- sd(bVals["aSstar", ])
    zVals <- bVals["z", ]
    ## Now use these zVals to get the lower and upper c values.
    cValues <- quantile(zVals, probs = c((1 - confidence)/2, (confidence + 
        1)/2), na.rm = TRUE)
    cLower <- min(cValues)
    cUpper <- max(cValues)
    interval <- c(lower = aPstar - cUpper * SDhat, middle = aPstar, upper = aPstar - 
        cLower * SDhat)
    return(interval)
}
```

